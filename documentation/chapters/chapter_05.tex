\section{Performance Evaluation}
\subsection{Bilingual Evaluation Understudy}
The Bilingual Evaluation Understudy (BLEU) algorithm evaluates the precision score of a candidate machine translation against a reference human translation. The reference translation is a assumed to be a model example of a translation. The algorithm identifies all of $n$-gram matches and evaluates the strength of the match with the precision score. The precision score is the fraction of $n$-grams in the translation that also appear in the reference.
\\\\
Let $k$ be the maximum n-gram to evaluate the score of translation. Let
\begin{equation}
\label{eq:5.1}
p_n = \frac{\# \; \text{matched} \; n\text{-grams}}{\# \; n\text{-grams} \; \text{in candidate translation}}
\end{equation}
the precision score for the grams of length $n$. 
\\\\
Finally, let $w_n = \frac{1}{2n}$  be a geometric weighting for the precision of the $n$'th gram. The brevity penalty is defined as:
\begin{equation}
\label{eq:5.2}
\beta = \exp ^ {min \left(0, 1 - \frac{len_{ref}}{len_{MT}} \right)}
\end{equation}
where $len_{ref}$ is the length of the reference translation and $len_{MT}$ is the length of the machine translation.
\\\\
The BLEU score is then defined as \cite{manning2019cs224n}:
\begin{equation}
\label{eq:5.3}
\text{BLEU} = \beta \prod _{i = 1} ^ k p_n ^{w_n}
\end{equation}

\section{Result and Analysis}
The BLEU score on training data is shown in Table \ref{table:5.1}.
\begin{table}[H]
\centering
\def\arraystretch{1.25}
\caption{BLEU score on training data}
\label{table:5.1}
\vspace{5pt}
\begin{tabular}{|c|c|c|} \hline
& \textbf{Simple NMT} & \textbf{NMT with Attention} \\ \hline
BLEU-1 & 0.031293 & 0.825670 \\ \hline
BLEU-2 & 0.089161 & 0.801728 \\ \hline
BLEU-3 & 0.135540 & 0.805319 \\ \hline
BLEU-4 & 0.150501 & 0.774398 \\ \hline
\end{tabular}
\end{table}
\
The BLEU score on test data is shown in Table \ref{table:5.2}.
\begin{table}[H]
\centering
\def\arraystretch{1.25}
\caption{BLEU score on test data}
\label{table:5.2}
\vspace{5pt}
\begin{tabular}{|c|c|c|} \hline
& \textbf{Simple NMT} & \textbf{NMT with Attention} \\ \hline
BLEU-1 & 0.030298 & 0.378832 \\ \hline
BLEU-2 & 0.086331 & 0.306991 \\ \hline
BLEU-3 & 0.131241 & 0.315862 \\ \hline
BLEU-4 & 0.145728 & 0.262754 \\ \hline
\end{tabular}
\end{table}
\
The BLEU scores show that NMT with Attention outperformed the simple NMT system in Nepali to English sentence level translation.