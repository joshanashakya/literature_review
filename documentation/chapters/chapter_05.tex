\section{Performance Evaluation}
\subsection{Bilingual Evaluation Understudy}
The Bilingual Evaluation Understudy (BLEU) algorithm evaluates the precision score of a candidate machine translation against a reference human translation. The reference translation is a assumed to be a model example of a translation. The algorithm identifies all of $n$-gram matches and evaluates the strength of the match with the precision score. The precision score is the fraction of $n$-grams in the translation that also appear in the reference.
\\\\
Let $k$ be the maximum n-gram to evaluate the score of translation. Let
\begin{equation}
\label{eq:5.1}
p_n = \frac{\# \; \text{matched} \; n\text{-grams}}{\# \; n\text{-grams} \; \text{in candidate translation}}
\end{equation}
the precision score for the grams of length $n$. 
\\\\
Finally, let $w_n = \frac{1}{2n}$  be a geometric weighting for the precision of the $n$'th gram. The brevity penalty is defined as:
\begin{equation}
\label{eq:5.2}
\beta = \exp ^ {min \left(0, 1 - \frac{len_{ref}}{len_{MT}} \right)}
\end{equation}
where $len_{ref}$ is the length of the reference translation and $len_{MT}$ is the length of the machine translation.
\\\\
The BLEU score is then defined as \cite{manning2019cs224n}:
\begin{equation}
\label{eq:5.3}
\text{BLEU} = \beta \prod _{i = 1} ^ k p_n ^{w_n}
\end{equation}
\newpage
\section{Result and Analysis}
The BLEU score on training data is shown in Table \ref{table:5.1}.
\begin{table}[H]
\centering
\def\arraystretch{1.25}
\caption{BLEU score on training data}
\label{table:5.1}
\vspace{5pt}
\begin{tabular}{|c|c|c|} \hline
& \textbf{Simple NMT} & \textbf{NMT with Attention} \\ \hline
BLEU-1 & 0.303226 & 0.788407 \\ \hline
BLEU-2 & 0.131711 & 0.756266 \\ \hline
BLEU-3 & 0.072301 & 0.758734 \\ \hline
BLEU-4 & 0.026194 & 0.720596 \\ \hline
\end{tabular}
\end{table}
\
The BLEU score on test data is shown in Table \ref{table:5.2}.
\begin{table}[H]
\centering
\def\arraystretch{1.25}
\caption{BLEU score on test data}
\label{table:5.2}
\vspace{5pt}
\begin{tabular}{|c|c|c|} \hline
& \textbf{Simple NMT} & \textbf{NMT with Attention} \\ \hline
BLEU-1 & 0.174330 & 0.370088 \\ \hline
BLEU-2 & 0.057952 & 0.294486 \\ \hline
BLEU-3 & 0.028397 & 0.301333 \\ \hline
BLEU-4 & 0.007851 & 0.247299 \\ \hline
\end{tabular}
\end{table}
\
The BLEU scores show that NMT with Attention outperformed the simple NMT system in Nepali to English sentence level translation.