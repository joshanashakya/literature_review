\section{Introduction}
The term “machine translation” (MT) refers to computerized systems responsible for an automatic translation from one language to another. The machine translation technology allows us to communicate effortlessly with those that speak a different language, or to understand the contents that are not in the users’ native language. The several different types of MT approaches are  rule-based, statistical, example-based, and neural. The progression in technologies has replaced the older systems with newer, more effective technologies. Up until late 2016 \cite{semantix_mt}, the products using MT technology were hugely based on statistical methods known as Statistical Machine Translation (SMT). The SMT technology uses advanced statistical analysis to obtain the best possible translations for a word provided the context of a few surrounding words. Among different approaches to MT technology, the most significant developments have been the advent of Neural Machine Translation (NMT) \cite{memsource_mt}. Unlike the SMT technology, the NMT aims at building a single neural network that can be jointly tuned to obtain maximum translation performance \cite{bahdanau2014neural}.
\\\\
Given a source sentence, $x_1, \ldots, x_n$, the neural machine translation system comprises of a neural network that directly models the conditional probability $p(y|x)$ of translating a source sentence to a target sentence, $y_1, \ldots, y_m$ \cite{luong2015effective}. In translation, the system maximizes the conditional probability, i.e., $\text{argmax} _y p(y|x)$ of the sentence pairs using a parallel training corpus. Once the distribution is learned, given a source sentence a corresponding translation can be generated by searching for the sentence that maximizes the conditional probability \cite{bahdanau2014neural}. 

\section{Sequence-to-Sequence Model}
A sequence-to-sequence model is an end-to-end model consisting of two components:
\begin{enumerate}[i.]
\item Encoder
\\
An encoder reads the source sentence, a sequence of vectors $x = (x_1, \ldots, x_n)$ and  generates a fixed-dimensional representation $s$ for the sequence. To accomplish this, the encoder reads the input tokens one at a time using a recurrent neural network cell such that
\begin{equation}
\label{eq:1.1}
h_t = f(h_{t-1}, x_t)
\end{equation}
and
\begin{equation}
\label{eq:1.2}
s = q(\{h_1, \ldots, h_n\})
\end{equation}
where 
$h_t \in \mathbb {R}$ is a hidden state at time $t$, and $s$ is a representation generated from the sequence of the hidden states. $f$ and $q$ are nonlinear functions. $f$ is an RNN unit and $q(\{h_1, \ldots, h_n\}) = h_n$ is the final hidden state of the cell.

\item Decoder
\\
The decoder is also a recurrent neural network whose hidden state of the first layer is initialized with the source representation $s$ from the encoder. It is trained to predict the next word $y_j$ given the representation $s$ and all the previously predicted words $\{y_1, \ldots, y_{j−1}\}$. That is, the decoder defines a probability over the translation $y$ by decomposing the joint probability into the ordered conditionals:
\begin{equation}
\label{eq:1.3}
\log p(y|x) = \sum_{j=1}^{m} \log p(y_j|y_{<j}, s)
\end{equation}
The probability of decoding each word $y_j$ is computed as:
\begin{equation}
\label{eq:1.4}
p(y_j|y_{<j}, s) = \text{softmax}(g(h_j))
\end{equation}
where, $g$ is the transformation function to produce a vocabulary-sized vector, $h_j$ is the RNN hidden unit such that
\begin{equation}
\label{eq:1.5}
h_j = f(h_{j-1}, s)
\end{equation}
where $f$ computes the current hidden state given the previous hidden state.
\end{enumerate}
\ \\
The token is appended to the end of the input to signify the decoder the start of the output generation. This is followed by a softmax on the final layer’s output to generate the first output word. Then, that word is passed into the first layer to repeat the generation. 
\newpage
The graphical depiction of the model is shown in figure \ref{fig:1.1}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{{seq-to-seq.jpg}}
\caption[Sequence-to-Sequence Model]{Sequence-to-Sequence Model \cite{giorgi_smam}.}
\label{fig:1.1}
\end{figure}
\
The training objective of the sequence-to-sequence model is formulated as:
\begin{equation}
\label{eq:1.6}
J_t = \sum_{(x, y) \in \mathbb{D}} -\log p(y|x)
\end{equation}
with $D$ being the parallel training corpus \cite{luong2015effective}.
\\\\
Once the output sequence is generated, the cross entropy loss of the model is minimized and back-propagation is performed. As both the encoder and decoder are trained at the same time, they both learn the same context vector representation \cite{manning2019cs224n,bahdanau2014neural}.

\section{Attention Mechanism}
Theoretically, a large enough and well-trained encoder-decoder model should be able to achieve perfect machine translation. However, in practice, the encoder-decoder attempts to store information about sentences of any arbitrary length in a hidden vector of fixed size. Thus, the network won’t be able to encode all of the information in the longer sentences to translate \cite{neubig2017neural}. The attention mechanism learns to assign significance to different parts of the input for each step of the output by providing the decoder network with a look at the entire input sequence at every decoding step; the decoder can then decide what input words are important at any point in time \cite{manning2019cs224n}. There are several types of attention mechanisms:
\begin{enumerate}[i.]
\item Bahdanau et al. NMT model
\item Luong et al. NMT model
\end{enumerate}

\subsection{Luong et al. NMT model}
In this model, during the decoding phase, at each time step $t$, the hidden state $h_t$ is taken as input to derive the context vector $c_t$. Given the target hidden state $h_t$ and the source-side context vector $c_t$, the model performs concatenation to combine the information from both vectors to produce attentional hidden state as follows:
\begin{equation}
\label{eq:1.7}
\tilde{h_t} = \tanh (W_c[c_t; h_t])
\end{equation}
The attentional vector $\tilde{h_t}$ is then fed through the softmax layer to produce the predictive distribution as:
\begin{equation}
\label{eq:1.8}
p(y_t|y_{<t}, x) = \text{softmax}(W_s \tilde{h_t})
\end{equation}
\
The source-side context vector $c_t$ can be computed in two ways:
\begin{enumerate}[i.]
\item Global Attention
\item Local Attention
\end{enumerate}

\subsubsection{Global Attention}
The global attention mechanism runs vanilla sequence-to-sequence model and considers all the hidden states of the encoder to derive the context vector $c_t$. A variable-length alignment vector of size equal to the number of time steps on the source side is derived using the current target hidden state $h_t$ with each source hidden state $\bar{h_s}$ as follows \cite{luong2015effective}:
\begin{align}
\label{eq:1.9}
a_t(s) & = align(h_t, \bar{h}_s) \\
& = \frac{\exp (\text{score}(h_t, \bar{h}_s))}{\sum_{s^\prime} \exp (\text{score}(h_t, \bar{h}_{s^\prime})} \nonumber
\end{align}
Here, one of the following scoring functions is used:
\begin{equation}
\label{eq:1.10}
  \text{score}(h_t, \bar{h}_s) = 
    \begin{cases}
      h_t^T \bar{h}_w & \text{dot} \\
      h_t^T W_a \bar{h}_s & \text{general} \\
      v_a^T \tanh (W_a[h_t; \bar{h}_s]) & \text{concat}
    \end{cases}       
\end{equation}
\
Given the alignment vectors as weights, the context vector $c_t$ is computed as:
\begin{equation}
\label{eq:1.11}
c_t = \sum _s a_t(s) \times \bar{h}_s
\end{equation}
\ 
This global attentional model is illustrated in figure \ref{fig:1.2}.
\begin{figure}[H]
\centering
\includegraphics[scale=0.8]{{global_attention_model.png}}
\caption[Global Attention Model]{Global Attention Model \cite{luong2015effective}.}
\label{fig:1.2}
\end{figure}

\section{Problem Statement}
The neural machine translation system can be constructed with vanilla sequence-to-sequence model. However, the encoder of such sequence-to-sequence model attempts to store sentences information of any arbitrary length in a hidden vector of fixed size. Thus, making the information incomplete for the decoder network. The attention mechanism provides the decoder the information about the different parts of the input for each step of the output.

\section{Objective}
The main objective of this literature review is to compare the performance of sequence-to-sequence model with and without attention mechanism in machine translation.