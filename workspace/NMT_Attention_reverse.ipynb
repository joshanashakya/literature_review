{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "NMT_Attention_reverse.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ3M01gBo_sA",
        "outputId": "54eb054e-92de-4037-a38f-7ad12cc5853b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "oQ3M01gBo_sA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6386033"
      },
      "source": [
        "### 1. Load dataset and Tokenize"
      ],
      "id": "f6386033"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "341dd2cc",
        "outputId": "8aab6026-5eb0-4294-9e2c-0d06a659205c"
      },
      "source": [
        "!pip install tensorflow-addons==0.11.2"
      ],
      "id": "341dd2cc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons==0.11.2\n",
            "  Downloading tensorflow_addons-0.11.2-cp37-cp37m-manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 26.0 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 29.6 MB/s eta 0:00:01\r\u001b[K     |█                               | 30 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 19.0 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 61 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 12.7 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 13.7 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 92 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███▋                            | 122 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████▌                           | 153 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 184 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 215 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 245 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 256 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 286 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 307 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 317 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 337 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 348 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 368 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 399 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 409 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 430 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 440 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 460 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 471 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 491 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 501 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 512 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 522 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 532 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 542 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 563 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 573 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 583 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 593 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 604 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 614 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 624 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 634 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 645 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 665 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 675 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 686 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 696 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 706 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 716 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 727 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 737 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 747 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 768 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 778 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 788 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 798 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 808 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 819 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 829 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 839 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 849 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 860 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 870 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 880 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 890 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 901 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 911 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 921 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 942 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 952 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 962 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 972 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 983 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 993 kB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.0 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.0 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.0 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.0 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.1 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.1 MB 11.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons==0.11.2) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35d98157",
        "outputId": "1ae74631-4e27-410a-807f-1f9b5de7d7f0"
      },
      "source": [
        "from pickle import load\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_addons as tfa\n",
        "import time\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import corpus_bleu"
      ],
      "id": "35d98157",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.6.0 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  UserWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1585c02c"
      },
      "source": [
        "# load a clean dataset\n",
        "def load_clean_sentences(filename):\n",
        "    return load(open(filename, 'rb'))\n",
        "\n",
        "loc = 'drive/MyDrive/nmt_test/'\n",
        "\n",
        "# load datasets\n",
        "dataset = load_clean_sentences(loc + 'english-nepali-both.pkl')\n",
        "train = load_clean_sentences(loc + 'english-nepali-train.pkl')\n",
        "test = load_clean_sentences(loc + 'english-nepali-test.pkl')"
      ],
      "id": "1585c02c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79134b0e"
      },
      "source": [
        "def create_dataset(dataset):\n",
        "    en = []\n",
        "    ne = []\n",
        "    for line in dataset:\n",
        "        words1 = line[0].split(' ')\n",
        "        words2 = line[1].split(' ')\n",
        "        words1.append('<end>')\n",
        "        words1.insert(0, '<start>')\n",
        "        words2.append('<end>')\n",
        "        words2.insert(0, '<start>')\n",
        "        en.append(words1)\n",
        "        ne.append(words2)\n",
        "    return en, ne"
      ],
      "id": "79134b0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ed67f71"
      },
      "source": [
        "def tokenize(lang):\n",
        "    # lang = list of sentences in a language\n",
        "    \n",
        "    # print(len(lang), \"example sentence: {}\".format(lang[0]))\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "    \n",
        "    ## tf.keras.preprocessing.text.Tokenizer.texts_to_sequences converts string (w1, w2, w3, ......, wn) \n",
        "    ## to a list of correspoding integer ids of words (id_w1, id_w2, id_w3, ...., id_wn)\n",
        "#     tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    \n",
        "    # tf.keras.preprocessing.sequence.pad_sequences takes argument a list of integer id sequences \n",
        "    ## and pads the sequences to match the longest sequences in the given input\n",
        "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
        "    \n",
        "    return lang_tokenizer"
      ],
      "id": "0ed67f71",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89692171"
      },
      "source": [
        "def tf_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "id": "89692171",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f06bb19b"
      },
      "source": [
        "# tokenize\n",
        "target_lang, input_lang = create_dataset(dataset)\n",
        "inp_lang_tokenizer = tokenize(input_lang)\n",
        "targ_lang_tokenizer = tokenize(target_lang)"
      ],
      "id": "f06bb19b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22309301"
      },
      "source": [
        "# load datasets\n",
        "def load_dataset(dataset):\n",
        "    targ_lang, inp_lang = create_dataset(train)\n",
        "#     input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "#     target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(inp_lang_tokenizer.texts_to_sequences(inp_lang),\n",
        "                                                                 padding='post')\n",
        "    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(targ_lang_tokenizer.texts_to_sequences(targ_lang),\n",
        "                                                                 padding='post')\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "id": "22309301",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e31397db"
      },
      "source": [
        "input_tensor_train, target_tensor_train, inp_lang, targ_lang = load_dataset(train)\n",
        "max_length_targ, max_length_inp = tf_max_length(target_tensor_train), tf_max_length(input_tensor_train)"
      ],
      "id": "e31397db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64a18b19"
      },
      "source": [
        "### 2. Train and Test Split"
      ],
      "id": "64a18b19"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54cc9c6c",
        "outputId": "6bdde8ed-0941-45ee-d314-64ef56c7cf90"
      },
      "source": [
        "# input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "print(\"Length\")\n",
        "print(\"input_tensor_train = {}\".format(len(input_tensor_train)))\n",
        "print(\"target_tensor_train = {}\".format(len(target_tensor_train)))"
      ],
      "id": "54cc9c6c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length\n",
            "input_tensor_train = 3000\n",
            "target_tensor_train = 3000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99a406a4"
      },
      "source": [
        "### 3. Index to Word Mapping"
      ],
      "id": "99a406a4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "659be3ec",
        "outputId": "38d6fb89-bb1e-4f73-e182-1686cc53a770"
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "    \n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "id": "659be3ec",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "3226 ----> सहरमा\n",
            "98 ----> निजी\n",
            "5698 ----> वाहन\n",
            "42 ----> प्रयोग\n",
            "7 ----> गर्न\n",
            "517 ----> प्रतिबन्ध\n",
            "1360 ----> लगाइएको\n",
            "12 ----> थियो।\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "2 ----> <start>\n",
            "120 ----> private\n",
            "2894 ----> vehicle\n",
            "98 ----> use\n",
            "16 ----> was\n",
            "2895 ----> banned\n",
            "6 ----> in\n",
            "1 ----> the\n",
            "771 ----> city\n",
            "3 ----> <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e638b68c"
      },
      "source": [
        "### 4. Some important parameters"
      ],
      "id": "e638b68c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ceb120f9",
        "outputId": "aee5a8e6-6d61-4d29-fa86-33efbfe79f21"
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Some important parameters\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1\n",
        "\n",
        "embedding_dim = 128\n",
        "units = 256\n",
        "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
        "\n",
        "print(\"Vocabulary Sizes\")\n",
        "print(\"vocab_inp_size = {}\".format(vocab_inp_size))\n",
        "print(\"vocab_tar_size = {}\".format(vocab_tar_size))"
      ],
      "id": "ceb120f9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Sizes\n",
            "vocab_inp_size = 10797\n",
            "vocab_tar_size = 6882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b90638d"
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "id": "3b90638d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42318a88"
      },
      "source": [
        "### 5. Encoder"
      ],
      "id": "42318a88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "430d4f50"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        ##-------- LSTM layer in Encoder ------- ##\n",
        "        self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
        "        return output, h, c\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))] "
      ],
      "id": "430d4f50",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93fb09eb"
      },
      "source": [
        "### 6. Decoder"
      ],
      "id": "93fb09eb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a02fbac7"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        # self.attention_type = attention_type\n",
        "\n",
        "        # Embedding Layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        #Final Dense layer on which softmax will be applied\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "        # Define the fundamental cell for decoder recurrent structure\n",
        "        self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "\n",
        "        # Sampler\n",
        "        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "        # Create attention mechanism with memory = None\n",
        "        self.attention_mechanism = self.build_attention_mechanism(self.dec_units, None, self.batch_sz*[max_length_inp])\n",
        "\n",
        "        # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
        "        self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "\n",
        "        # Define the decoder with respect to fundamental rnn cell\n",
        "        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "    def build_rnn_cell(self, batch_sz):\n",
        "        rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
        "                                      self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "        return rnn_cell\n",
        "    \n",
        "    def build_attention_mechanism(self, dec_units, memory, memory_sequence_length):\n",
        "        # ------------- #\n",
        "        # typ: Which sort of attention (Bahdanau, Luong)\n",
        "        # dec_units: final dimension of attention outputs \n",
        "        # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "        # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "        return tfa.seq2seq.LuongAttention(units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "    def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "        decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "        return decoder_initial_state\n",
        "    \n",
        "    def call(self, inputs, initial_state):\n",
        "        x = self.embedding(inputs)\n",
        "        outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_targ-1])\n",
        "        return outputs\n"
      ],
      "id": "a02fbac7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cc51827"
      },
      "source": [
        "### 7. Define the optimizer and the loss function"
      ],
      "id": "1cc51827"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cc853f6"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    # real shape = (BATCH_SIZE, max_length_output)\n",
        "    # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "    cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "    loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "    mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "    loss = mask* loss\n",
        "    loss = tf.reduce_mean(loss)\n",
        "    return loss  "
      ],
      "id": "4cc853f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "865cec96"
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
      ],
      "id": "865cec96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce17f46f"
      },
      "source": [
        "import os\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "id": "ce17f46f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fefab652"
      },
      "source": [
        "### 8. Train"
      ],
      "id": "fefab652"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4db7be0b"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "\n",
        "        dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "        real = targ[ : , 1: ]       # ignore <start> token\n",
        "\n",
        "        # Set the AttentionMechanism object with encoder_outputs\n",
        "        decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "        # Create AttentionWrapperState as initial_state for decoder\n",
        "        decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "        pred = decoder(dec_input, decoder_initial_state)\n",
        "        logits = pred.rnn_output\n",
        "        loss = loss_function(real, logits)\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "    return loss"
      ],
      "id": "4db7be0b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fbc5416",
        "outputId": "c4217081-94a2-44bd-977b-c14d83cf2c01"
      },
      "source": [
        "EPOCHS = 500\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "    # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "        \n",
        "        print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "        print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "id": "8fbc5416",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 5.0079\n",
            "Epoch 2 Batch 0 Loss 3.3390\n",
            "Epoch 2 Loss 3.3860\n",
            "Time taken for 1 epoch 11.396674156188965 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 3.1297\n",
            "Epoch 4 Batch 0 Loss 3.1831\n",
            "Epoch 4 Loss 3.1977\n",
            "Time taken for 1 epoch 11.379096269607544 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 3.2662\n",
            "Epoch 6 Batch 0 Loss 2.9294\n",
            "Epoch 6 Loss 2.9767\n",
            "Time taken for 1 epoch 11.310560703277588 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 2.8152\n",
            "Epoch 8 Batch 0 Loss 2.7645\n",
            "Epoch 8 Loss 2.7614\n",
            "Time taken for 1 epoch 11.346597671508789 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 2.7628\n",
            "Epoch 10 Batch 0 Loss 2.4254\n",
            "Epoch 10 Loss 2.5485\n",
            "Time taken for 1 epoch 11.393122434616089 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 2.4091\n",
            "Epoch 12 Batch 0 Loss 2.1541\n",
            "Epoch 12 Loss 2.3734\n",
            "Time taken for 1 epoch 11.317896842956543 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.9386\n",
            "Epoch 14 Batch 0 Loss 2.0259\n",
            "Epoch 14 Loss 2.2097\n",
            "Time taken for 1 epoch 11.412219047546387 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.9235\n",
            "Epoch 16 Batch 0 Loss 2.1218\n",
            "Epoch 16 Loss 2.0551\n",
            "Time taken for 1 epoch 11.370058298110962 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 2.2065\n",
            "Epoch 18 Batch 0 Loss 1.7746\n",
            "Epoch 18 Loss 1.8903\n",
            "Time taken for 1 epoch 11.408844470977783 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.8198\n",
            "Epoch 20 Batch 0 Loss 1.7587\n",
            "Epoch 20 Loss 1.7315\n",
            "Time taken for 1 epoch 11.447773218154907 sec\n",
            "\n",
            "Epoch 21 Batch 0 Loss 1.5475\n",
            "Epoch 22 Batch 0 Loss 1.6543\n",
            "Epoch 22 Loss 1.5773\n",
            "Time taken for 1 epoch 11.425647974014282 sec\n",
            "\n",
            "Epoch 23 Batch 0 Loss 1.4556\n",
            "Epoch 24 Batch 0 Loss 1.4494\n",
            "Epoch 24 Loss 1.4418\n",
            "Time taken for 1 epoch 11.343445062637329 sec\n",
            "\n",
            "Epoch 25 Batch 0 Loss 1.1913\n",
            "Epoch 26 Batch 0 Loss 1.2762\n",
            "Epoch 26 Loss 1.3119\n",
            "Time taken for 1 epoch 11.421742916107178 sec\n",
            "\n",
            "Epoch 27 Batch 0 Loss 1.2719\n",
            "Epoch 28 Batch 0 Loss 1.0326\n",
            "Epoch 28 Loss 1.1994\n",
            "Time taken for 1 epoch 11.345184087753296 sec\n",
            "\n",
            "Epoch 29 Batch 0 Loss 1.1086\n",
            "Epoch 30 Batch 0 Loss 1.0596\n",
            "Epoch 30 Loss 1.0818\n",
            "Time taken for 1 epoch 11.343132019042969 sec\n",
            "\n",
            "Epoch 31 Batch 0 Loss 1.0073\n",
            "Epoch 32 Batch 0 Loss 0.9241\n",
            "Epoch 32 Loss 0.9905\n",
            "Time taken for 1 epoch 11.416588306427002 sec\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.9300\n",
            "Epoch 34 Batch 0 Loss 0.8681\n",
            "Epoch 34 Loss 0.9067\n",
            "Time taken for 1 epoch 11.358522653579712 sec\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.9171\n",
            "Epoch 36 Batch 0 Loss 0.7859\n",
            "Epoch 36 Loss 0.8218\n",
            "Time taken for 1 epoch 11.403805494308472 sec\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.7264\n",
            "Epoch 38 Batch 0 Loss 0.6965\n",
            "Epoch 38 Loss 0.7444\n",
            "Time taken for 1 epoch 11.377346754074097 sec\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.6787\n",
            "Epoch 40 Batch 0 Loss 0.5981\n",
            "Epoch 40 Loss 0.6684\n",
            "Time taken for 1 epoch 11.424093246459961 sec\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.5951\n",
            "Epoch 42 Batch 0 Loss 0.4999\n",
            "Epoch 42 Loss 0.6001\n",
            "Time taken for 1 epoch 11.317407131195068 sec\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.5242\n",
            "Epoch 44 Batch 0 Loss 0.4471\n",
            "Epoch 44 Loss 0.5439\n",
            "Time taken for 1 epoch 11.339933395385742 sec\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.5055\n",
            "Epoch 46 Batch 0 Loss 0.5199\n",
            "Epoch 46 Loss 0.4911\n",
            "Time taken for 1 epoch 11.413509130477905 sec\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.4714\n",
            "Epoch 48 Batch 0 Loss 0.4773\n",
            "Epoch 48 Loss 0.4327\n",
            "Time taken for 1 epoch 11.306666374206543 sec\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.3590\n",
            "Epoch 50 Batch 0 Loss 0.3846\n",
            "Epoch 50 Loss 0.4052\n",
            "Time taken for 1 epoch 11.249475002288818 sec\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.3545\n",
            "Epoch 52 Batch 0 Loss 0.2511\n",
            "Epoch 52 Loss 0.3715\n",
            "Time taken for 1 epoch 11.35751223564148 sec\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.3449\n",
            "Epoch 54 Batch 0 Loss 0.2682\n",
            "Epoch 54 Loss 0.3067\n",
            "Time taken for 1 epoch 11.385879039764404 sec\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2737\n",
            "Epoch 56 Batch 0 Loss 0.2090\n",
            "Epoch 56 Loss 0.2689\n",
            "Time taken for 1 epoch 11.408252954483032 sec\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.2142\n",
            "Epoch 58 Batch 0 Loss 0.2814\n",
            "Epoch 58 Loss 0.2443\n",
            "Time taken for 1 epoch 11.370916843414307 sec\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2012\n",
            "Epoch 60 Batch 0 Loss 0.1818\n",
            "Epoch 60 Loss 0.2134\n",
            "Time taken for 1 epoch 11.405556917190552 sec\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.1569\n",
            "Epoch 62 Batch 0 Loss 0.1669\n",
            "Epoch 62 Loss 0.1976\n",
            "Time taken for 1 epoch 11.344600439071655 sec\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.1982\n",
            "Epoch 64 Batch 0 Loss 0.1634\n",
            "Epoch 64 Loss 0.1800\n",
            "Time taken for 1 epoch 11.381227493286133 sec\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.1321\n",
            "Epoch 66 Batch 0 Loss 0.1330\n",
            "Epoch 66 Loss 0.1678\n",
            "Time taken for 1 epoch 11.324475526809692 sec\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.1205\n",
            "Epoch 68 Batch 0 Loss 0.1244\n",
            "Epoch 68 Loss 0.1514\n",
            "Time taken for 1 epoch 11.446067094802856 sec\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.1440\n",
            "Epoch 70 Batch 0 Loss 0.1399\n",
            "Epoch 70 Loss 0.1283\n",
            "Time taken for 1 epoch 11.356974363327026 sec\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.0859\n",
            "Epoch 72 Batch 0 Loss 0.0827\n",
            "Epoch 72 Loss 0.0928\n",
            "Time taken for 1 epoch 11.40506625175476 sec\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.0761\n",
            "Epoch 74 Batch 0 Loss 0.0585\n",
            "Epoch 74 Loss 0.0583\n",
            "Time taken for 1 epoch 11.398326635360718 sec\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.0449\n",
            "Epoch 76 Batch 0 Loss 0.0340\n",
            "Epoch 76 Loss 0.0356\n",
            "Time taken for 1 epoch 11.382469654083252 sec\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.0259\n",
            "Epoch 78 Batch 0 Loss 0.0284\n",
            "Epoch 78 Loss 0.0286\n",
            "Time taken for 1 epoch 11.280198812484741 sec\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.0232\n",
            "Epoch 80 Batch 0 Loss 0.0190\n",
            "Epoch 80 Loss 0.0200\n",
            "Time taken for 1 epoch 11.446394920349121 sec\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.0154\n",
            "Epoch 82 Batch 0 Loss 0.0117\n",
            "Epoch 82 Loss 0.0146\n",
            "Time taken for 1 epoch 11.386542320251465 sec\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.0116\n",
            "Epoch 84 Batch 0 Loss 0.0109\n",
            "Epoch 84 Loss 0.0111\n",
            "Time taken for 1 epoch 11.409568071365356 sec\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.0089\n",
            "Epoch 86 Batch 0 Loss 0.0102\n",
            "Epoch 86 Loss 0.0093\n",
            "Time taken for 1 epoch 11.38434386253357 sec\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.0096\n",
            "Epoch 88 Batch 0 Loss 0.0073\n",
            "Epoch 88 Loss 0.0082\n",
            "Time taken for 1 epoch 11.322126865386963 sec\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.0075\n",
            "Epoch 90 Batch 0 Loss 0.0071\n",
            "Epoch 90 Loss 0.0073\n",
            "Time taken for 1 epoch 11.37289547920227 sec\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.0054\n",
            "Epoch 92 Batch 0 Loss 0.0071\n",
            "Epoch 92 Loss 0.0066\n",
            "Time taken for 1 epoch 11.380880355834961 sec\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.0072\n",
            "Epoch 94 Batch 0 Loss 0.0060\n",
            "Epoch 94 Loss 0.0100\n",
            "Time taken for 1 epoch 11.444820404052734 sec\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.0379\n",
            "Epoch 96 Batch 0 Loss 0.6982\n",
            "Epoch 96 Loss 0.8482\n",
            "Time taken for 1 epoch 11.32681941986084 sec\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.5519\n",
            "Epoch 98 Batch 0 Loss 0.3300\n",
            "Epoch 98 Loss 0.3126\n",
            "Time taken for 1 epoch 11.360406637191772 sec\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.1744\n",
            "Epoch 100 Batch 0 Loss 0.0969\n",
            "Epoch 100 Loss 0.1118\n",
            "Time taken for 1 epoch 11.32738709449768 sec\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.0859\n",
            "Epoch 102 Batch 0 Loss 0.0502\n",
            "Epoch 102 Loss 0.0438\n",
            "Time taken for 1 epoch 11.312939167022705 sec\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.0269\n",
            "Epoch 104 Batch 0 Loss 0.0212\n",
            "Epoch 104 Loss 0.0205\n",
            "Time taken for 1 epoch 11.421825885772705 sec\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.0154\n",
            "Epoch 106 Batch 0 Loss 0.0117\n",
            "Epoch 106 Loss 0.0134\n",
            "Time taken for 1 epoch 11.323655128479004 sec\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.0096\n",
            "Epoch 108 Batch 0 Loss 0.0086\n",
            "Epoch 108 Loss 0.0103\n",
            "Time taken for 1 epoch 11.383368730545044 sec\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.0075\n",
            "Epoch 110 Batch 0 Loss 0.0083\n",
            "Epoch 110 Loss 0.0086\n",
            "Time taken for 1 epoch 11.354807615280151 sec\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.0073\n",
            "Epoch 112 Batch 0 Loss 0.0074\n",
            "Epoch 112 Loss 0.0074\n",
            "Time taken for 1 epoch 11.352561712265015 sec\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.0067\n",
            "Epoch 114 Batch 0 Loss 0.0062\n",
            "Epoch 114 Loss 0.0064\n",
            "Time taken for 1 epoch 11.405788898468018 sec\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.0062\n",
            "Epoch 116 Batch 0 Loss 0.0052\n",
            "Epoch 116 Loss 0.0058\n",
            "Time taken for 1 epoch 11.339510917663574 sec\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.0055\n",
            "Epoch 118 Batch 0 Loss 0.0050\n",
            "Epoch 118 Loss 0.0052\n",
            "Time taken for 1 epoch 11.327145099639893 sec\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.0044\n",
            "Epoch 120 Batch 0 Loss 0.0042\n",
            "Epoch 120 Loss 0.0046\n",
            "Time taken for 1 epoch 11.37609577178955 sec\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.0044\n",
            "Epoch 122 Batch 0 Loss 0.0045\n",
            "Epoch 122 Loss 0.0042\n",
            "Time taken for 1 epoch 11.364430904388428 sec\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.0044\n",
            "Epoch 124 Batch 0 Loss 0.0042\n",
            "Epoch 124 Loss 0.0039\n",
            "Time taken for 1 epoch 11.3265540599823 sec\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.0034\n",
            "Epoch 126 Batch 0 Loss 0.0034\n",
            "Epoch 126 Loss 0.0036\n",
            "Time taken for 1 epoch 11.437561750411987 sec\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.0042\n",
            "Epoch 128 Batch 0 Loss 0.0031\n",
            "Epoch 128 Loss 0.0033\n",
            "Time taken for 1 epoch 11.3827645778656 sec\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.0027\n",
            "Epoch 130 Batch 0 Loss 0.0030\n",
            "Epoch 130 Loss 0.0030\n",
            "Time taken for 1 epoch 11.3766450881958 sec\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.0025\n",
            "Epoch 132 Batch 0 Loss 0.0030\n",
            "Epoch 132 Loss 0.0027\n",
            "Time taken for 1 epoch 11.342292785644531 sec\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.0025\n",
            "Epoch 134 Batch 0 Loss 0.0021\n",
            "Epoch 134 Loss 0.0026\n",
            "Time taken for 1 epoch 11.46699857711792 sec\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.0043\n",
            "Epoch 136 Batch 0 Loss 0.0025\n",
            "Epoch 136 Loss 0.0024\n",
            "Time taken for 1 epoch 11.375255584716797 sec\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.0026\n",
            "Epoch 138 Batch 0 Loss 0.0020\n",
            "Epoch 138 Loss 0.0023\n",
            "Time taken for 1 epoch 11.391445398330688 sec\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.0020\n",
            "Epoch 140 Batch 0 Loss 0.0018\n",
            "Epoch 140 Loss 0.0022\n",
            "Time taken for 1 epoch 11.426036357879639 sec\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.0022\n",
            "Epoch 142 Batch 0 Loss 0.0016\n",
            "Epoch 142 Loss 0.0020\n",
            "Time taken for 1 epoch 11.282249212265015 sec\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.0018\n",
            "Epoch 144 Batch 0 Loss 0.0018\n",
            "Epoch 144 Loss 0.0019\n",
            "Time taken for 1 epoch 11.40605902671814 sec\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.0019\n",
            "Epoch 146 Batch 0 Loss 0.0017\n",
            "Epoch 146 Loss 0.0018\n",
            "Time taken for 1 epoch 11.347306489944458 sec\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.0013\n",
            "Epoch 148 Batch 0 Loss 0.0014\n",
            "Epoch 148 Loss 0.0016\n",
            "Time taken for 1 epoch 11.346247673034668 sec\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.0016\n",
            "Epoch 150 Batch 0 Loss 0.0015\n",
            "Epoch 150 Loss 0.0015\n",
            "Time taken for 1 epoch 11.292358160018921 sec\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.0012\n",
            "Epoch 152 Batch 0 Loss 0.0032\n",
            "Epoch 152 Loss 0.0015\n",
            "Time taken for 1 epoch 11.294831275939941 sec\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.0014\n",
            "Epoch 154 Batch 0 Loss 0.0014\n",
            "Epoch 154 Loss 0.0054\n",
            "Time taken for 1 epoch 11.330612421035767 sec\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.0658\n",
            "Epoch 156 Batch 0 Loss 1.0709\n",
            "Epoch 156 Loss 1.0411\n",
            "Time taken for 1 epoch 11.331823587417603 sec\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.6636\n",
            "Epoch 158 Batch 0 Loss 0.2734\n",
            "Epoch 158 Loss 0.2763\n",
            "Time taken for 1 epoch 11.332675218582153 sec\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.1690\n",
            "Epoch 160 Batch 0 Loss 0.0956\n",
            "Epoch 160 Loss 0.0906\n",
            "Time taken for 1 epoch 11.344062566757202 sec\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.0427\n",
            "Epoch 162 Batch 0 Loss 0.0357\n",
            "Epoch 162 Loss 0.0326\n",
            "Time taken for 1 epoch 11.381266355514526 sec\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.0234\n",
            "Epoch 164 Batch 0 Loss 0.0161\n",
            "Epoch 164 Loss 0.0159\n",
            "Time taken for 1 epoch 11.365048170089722 sec\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.0105\n",
            "Epoch 166 Batch 0 Loss 0.0092\n",
            "Epoch 166 Loss 0.0102\n",
            "Time taken for 1 epoch 11.420579433441162 sec\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.0080\n",
            "Epoch 168 Batch 0 Loss 0.0076\n",
            "Epoch 168 Loss 0.0080\n",
            "Time taken for 1 epoch 11.437347173690796 sec\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.0070\n",
            "Epoch 170 Batch 0 Loss 0.0064\n",
            "Epoch 170 Loss 0.0067\n",
            "Time taken for 1 epoch 11.3630211353302 sec\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.0056\n",
            "Epoch 172 Batch 0 Loss 0.0062\n",
            "Epoch 172 Loss 0.0057\n",
            "Time taken for 1 epoch 11.354841709136963 sec\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.0050\n",
            "Epoch 174 Batch 0 Loss 0.0051\n",
            "Epoch 174 Loss 0.0050\n",
            "Time taken for 1 epoch 11.351999282836914 sec\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.0039\n",
            "Epoch 176 Batch 0 Loss 0.0042\n",
            "Epoch 176 Loss 0.0044\n",
            "Time taken for 1 epoch 11.242028951644897 sec\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.0039\n",
            "Epoch 178 Batch 0 Loss 0.0041\n",
            "Epoch 178 Loss 0.0040\n",
            "Time taken for 1 epoch 11.429218292236328 sec\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.0036\n",
            "Epoch 180 Batch 0 Loss 0.0036\n",
            "Epoch 180 Loss 0.0036\n",
            "Time taken for 1 epoch 11.429047584533691 sec\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.0034\n",
            "Epoch 182 Batch 0 Loss 0.0033\n",
            "Epoch 182 Loss 0.0033\n",
            "Time taken for 1 epoch 11.289547443389893 sec\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.0029\n",
            "Epoch 184 Batch 0 Loss 0.0033\n",
            "Epoch 184 Loss 0.0030\n",
            "Time taken for 1 epoch 11.402450799942017 sec\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.0026\n",
            "Epoch 186 Batch 0 Loss 0.0026\n",
            "Epoch 186 Loss 0.0027\n",
            "Time taken for 1 epoch 11.331610202789307 sec\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.0026\n",
            "Epoch 188 Batch 0 Loss 0.0021\n",
            "Epoch 188 Loss 0.0025\n",
            "Time taken for 1 epoch 11.34943175315857 sec\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.0022\n",
            "Epoch 190 Batch 0 Loss 0.0022\n",
            "Epoch 190 Loss 0.0023\n",
            "Time taken for 1 epoch 11.390938758850098 sec\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.0026\n",
            "Epoch 192 Batch 0 Loss 0.0023\n",
            "Epoch 192 Loss 0.0022\n",
            "Time taken for 1 epoch 11.36357855796814 sec\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.0019\n",
            "Epoch 194 Batch 0 Loss 0.0021\n",
            "Epoch 194 Loss 0.0020\n",
            "Time taken for 1 epoch 11.34552550315857 sec\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.0024\n",
            "Epoch 196 Batch 0 Loss 0.0016\n",
            "Epoch 196 Loss 0.0019\n",
            "Time taken for 1 epoch 11.418082475662231 sec\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.0017\n",
            "Epoch 198 Batch 0 Loss 0.0018\n",
            "Epoch 198 Loss 0.0017\n",
            "Time taken for 1 epoch 11.340622901916504 sec\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.0015\n",
            "Epoch 200 Batch 0 Loss 0.0014\n",
            "Epoch 200 Loss 0.0017\n",
            "Time taken for 1 epoch 11.371054887771606 sec\n",
            "\n",
            "Epoch 201 Batch 0 Loss 0.0015\n",
            "Epoch 202 Batch 0 Loss 0.0015\n",
            "Epoch 202 Loss 0.0016\n",
            "Time taken for 1 epoch 11.395732164382935 sec\n",
            "\n",
            "Epoch 203 Batch 0 Loss 0.0015\n",
            "Epoch 204 Batch 0 Loss 0.0013\n",
            "Epoch 204 Loss 0.0015\n",
            "Time taken for 1 epoch 20.592615365982056 sec\n",
            "\n",
            "Epoch 205 Batch 0 Loss 0.0012\n",
            "Epoch 206 Batch 0 Loss 0.0012\n",
            "Epoch 206 Loss 0.0014\n",
            "Time taken for 1 epoch 11.391292095184326 sec\n",
            "\n",
            "Epoch 207 Batch 0 Loss 0.0010\n",
            "Epoch 208 Batch 0 Loss 0.0012\n",
            "Epoch 208 Loss 0.0013\n",
            "Time taken for 1 epoch 11.459957361221313 sec\n",
            "\n",
            "Epoch 209 Batch 0 Loss 0.0011\n",
            "Epoch 210 Batch 0 Loss 0.0011\n",
            "Epoch 210 Loss 0.0013\n",
            "Time taken for 1 epoch 11.337043523788452 sec\n",
            "\n",
            "Epoch 211 Batch 0 Loss 0.0012\n",
            "Epoch 212 Batch 0 Loss 0.0011\n",
            "Epoch 212 Loss 0.0012\n",
            "Time taken for 1 epoch 11.355380773544312 sec\n",
            "\n",
            "Epoch 213 Batch 0 Loss 0.0012\n",
            "Epoch 214 Batch 0 Loss 0.0030\n",
            "Epoch 214 Loss 0.0012\n",
            "Time taken for 1 epoch 11.386601686477661 sec\n",
            "\n",
            "Epoch 215 Batch 0 Loss 0.0009\n",
            "Epoch 216 Batch 0 Loss 0.0026\n",
            "Epoch 216 Loss 0.0011\n",
            "Time taken for 1 epoch 11.396719932556152 sec\n",
            "\n",
            "Epoch 217 Batch 0 Loss 0.0010\n",
            "Epoch 218 Batch 0 Loss 0.0010\n",
            "Epoch 218 Loss 0.0010\n",
            "Time taken for 1 epoch 11.322219371795654 sec\n",
            "\n",
            "Epoch 219 Batch 0 Loss 0.0008\n",
            "Epoch 220 Batch 0 Loss 0.0008\n",
            "Epoch 220 Loss 0.0010\n",
            "Time taken for 1 epoch 11.317596912384033 sec\n",
            "\n",
            "Epoch 221 Batch 0 Loss 0.0011\n",
            "Epoch 222 Batch 0 Loss 0.0030\n",
            "Epoch 222 Loss 0.0011\n",
            "Time taken for 1 epoch 11.36992597579956 sec\n",
            "\n",
            "Epoch 223 Batch 0 Loss 0.0009\n",
            "Epoch 224 Batch 0 Loss 0.0008\n",
            "Epoch 224 Loss 0.0010\n",
            "Time taken for 1 epoch 11.381501913070679 sec\n",
            "\n",
            "Epoch 225 Batch 0 Loss 0.0008\n",
            "Epoch 226 Batch 0 Loss 0.0008\n",
            "Epoch 226 Loss 0.0009\n",
            "Time taken for 1 epoch 11.374979972839355 sec\n",
            "\n",
            "Epoch 227 Batch 0 Loss 0.0007\n",
            "Epoch 228 Batch 0 Loss 0.0006\n",
            "Epoch 228 Loss 0.0008\n",
            "Time taken for 1 epoch 11.346587419509888 sec\n",
            "\n",
            "Epoch 229 Batch 0 Loss 0.0006\n",
            "Epoch 230 Batch 0 Loss 0.0006\n",
            "Epoch 230 Loss 0.0008\n",
            "Time taken for 1 epoch 11.365856647491455 sec\n",
            "\n",
            "Epoch 231 Batch 0 Loss 0.0006\n",
            "Epoch 232 Batch 0 Loss 0.0006\n",
            "Epoch 232 Loss 0.0008\n",
            "Time taken for 1 epoch 11.332697868347168 sec\n",
            "\n",
            "Epoch 233 Batch 0 Loss 0.0006\n",
            "Epoch 234 Batch 0 Loss 0.0005\n",
            "Epoch 234 Loss 0.0008\n",
            "Time taken for 1 epoch 11.359365224838257 sec\n",
            "\n",
            "Epoch 235 Batch 0 Loss 0.0006\n",
            "Epoch 236 Batch 0 Loss 0.0005\n",
            "Epoch 236 Loss 0.0010\n",
            "Time taken for 1 epoch 11.392360925674438 sec\n",
            "\n",
            "Epoch 237 Batch 0 Loss 0.0006\n",
            "Epoch 238 Batch 0 Loss 0.0006\n",
            "Epoch 238 Loss 0.0007\n",
            "Time taken for 1 epoch 11.386870861053467 sec\n",
            "\n",
            "Epoch 239 Batch 0 Loss 0.0005\n",
            "Epoch 240 Batch 0 Loss 0.0005\n",
            "Epoch 240 Loss 0.0007\n",
            "Time taken for 1 epoch 11.434397459030151 sec\n",
            "\n",
            "Epoch 241 Batch 0 Loss 0.0005\n",
            "Epoch 242 Batch 0 Loss 0.0005\n",
            "Epoch 242 Loss 0.0007\n",
            "Time taken for 1 epoch 11.432624101638794 sec\n",
            "\n",
            "Epoch 243 Batch 0 Loss 0.0005\n",
            "Epoch 244 Batch 0 Loss 0.0005\n",
            "Epoch 244 Loss 0.0989\n",
            "Time taken for 1 epoch 11.415555953979492 sec\n",
            "\n",
            "Epoch 245 Batch 0 Loss 0.8137\n",
            "Epoch 246 Batch 0 Loss 1.0335\n",
            "Epoch 246 Loss 0.7945\n",
            "Time taken for 1 epoch 11.34442138671875 sec\n",
            "\n",
            "Epoch 247 Batch 0 Loss 0.4332\n",
            "Epoch 248 Batch 0 Loss 0.2202\n",
            "Epoch 248 Loss 0.1913\n",
            "Time taken for 1 epoch 11.340669631958008 sec\n",
            "\n",
            "Epoch 249 Batch 0 Loss 0.1109\n",
            "Epoch 250 Batch 0 Loss 0.0510\n",
            "Epoch 250 Loss 0.0604\n",
            "Time taken for 1 epoch 11.357674360275269 sec\n",
            "\n",
            "Epoch 251 Batch 0 Loss 0.0338\n",
            "Epoch 252 Batch 0 Loss 0.0203\n",
            "Epoch 252 Loss 0.0217\n",
            "Time taken for 1 epoch 11.422775506973267 sec\n",
            "\n",
            "Epoch 253 Batch 0 Loss 0.0139\n",
            "Epoch 254 Batch 0 Loss 0.0123\n",
            "Epoch 254 Loss 0.0107\n",
            "Time taken for 1 epoch 11.272577285766602 sec\n",
            "\n",
            "Epoch 255 Batch 0 Loss 0.0098\n",
            "Epoch 256 Batch 0 Loss 0.0075\n",
            "Epoch 256 Loss 0.0076\n",
            "Time taken for 1 epoch 11.34416127204895 sec\n",
            "\n",
            "Epoch 257 Batch 0 Loss 0.0069\n",
            "Epoch 258 Batch 0 Loss 0.0075\n",
            "Epoch 258 Loss 0.0061\n",
            "Time taken for 1 epoch 11.368753433227539 sec\n",
            "\n",
            "Epoch 259 Batch 0 Loss 0.0052\n",
            "Epoch 260 Batch 0 Loss 0.0046\n",
            "Epoch 260 Loss 0.0052\n",
            "Time taken for 1 epoch 11.44856882095337 sec\n",
            "\n",
            "Epoch 261 Batch 0 Loss 0.0050\n",
            "Epoch 262 Batch 0 Loss 0.0040\n",
            "Epoch 262 Loss 0.0045\n",
            "Time taken for 1 epoch 11.406440496444702 sec\n",
            "\n",
            "Epoch 263 Batch 0 Loss 0.0042\n",
            "Epoch 264 Batch 0 Loss 0.0040\n",
            "Epoch 264 Loss 0.0040\n",
            "Time taken for 1 epoch 11.341447114944458 sec\n",
            "\n",
            "Epoch 265 Batch 0 Loss 0.0037\n",
            "Epoch 266 Batch 0 Loss 0.0041\n",
            "Epoch 266 Loss 0.0035\n",
            "Time taken for 1 epoch 11.293081521987915 sec\n",
            "\n",
            "Epoch 267 Batch 0 Loss 0.0036\n",
            "Epoch 268 Batch 0 Loss 0.0029\n",
            "Epoch 268 Loss 0.0032\n",
            "Time taken for 1 epoch 11.390703201293945 sec\n",
            "\n",
            "Epoch 269 Batch 0 Loss 0.0037\n",
            "Epoch 270 Batch 0 Loss 0.0024\n",
            "Epoch 270 Loss 0.0029\n",
            "Time taken for 1 epoch 11.324145555496216 sec\n",
            "\n",
            "Epoch 271 Batch 0 Loss 0.0028\n",
            "Epoch 272 Batch 0 Loss 0.0027\n",
            "Epoch 272 Loss 0.0026\n",
            "Time taken for 1 epoch 11.415281772613525 sec\n",
            "\n",
            "Epoch 273 Batch 0 Loss 0.0024\n",
            "Epoch 274 Batch 0 Loss 0.0020\n",
            "Epoch 274 Loss 0.0024\n",
            "Time taken for 1 epoch 11.341725587844849 sec\n",
            "\n",
            "Epoch 275 Batch 0 Loss 0.0026\n",
            "Epoch 276 Batch 0 Loss 0.0023\n",
            "Epoch 276 Loss 0.0022\n",
            "Time taken for 1 epoch 11.398275375366211 sec\n",
            "\n",
            "Epoch 277 Batch 0 Loss 0.0024\n",
            "Epoch 278 Batch 0 Loss 0.0020\n",
            "Epoch 278 Loss 0.0020\n",
            "Time taken for 1 epoch 11.35077691078186 sec\n",
            "\n",
            "Epoch 279 Batch 0 Loss 0.0019\n",
            "Epoch 280 Batch 0 Loss 0.0016\n",
            "Epoch 280 Loss 0.0019\n",
            "Time taken for 1 epoch 11.363296508789062 sec\n",
            "\n",
            "Epoch 281 Batch 0 Loss 0.0018\n",
            "Epoch 282 Batch 0 Loss 0.0020\n",
            "Epoch 282 Loss 0.0018\n",
            "Time taken for 1 epoch 11.38485074043274 sec\n",
            "\n",
            "Epoch 283 Batch 0 Loss 0.0018\n",
            "Epoch 284 Batch 0 Loss 0.0015\n",
            "Epoch 284 Loss 0.0017\n",
            "Time taken for 1 epoch 20.597665071487427 sec\n",
            "\n",
            "Epoch 285 Batch 0 Loss 0.0014\n",
            "Epoch 286 Batch 0 Loss 0.0015\n",
            "Epoch 286 Loss 0.0016\n",
            "Time taken for 1 epoch 11.419817686080933 sec\n",
            "\n",
            "Epoch 287 Batch 0 Loss 0.0015\n",
            "Epoch 288 Batch 0 Loss 0.0012\n",
            "Epoch 288 Loss 0.0014\n",
            "Time taken for 1 epoch 11.424976825714111 sec\n",
            "\n",
            "Epoch 289 Batch 0 Loss 0.0013\n",
            "Epoch 290 Batch 0 Loss 0.0013\n",
            "Epoch 290 Loss 0.0014\n",
            "Time taken for 1 epoch 11.38166856765747 sec\n",
            "\n",
            "Epoch 291 Batch 0 Loss 0.0013\n",
            "Epoch 292 Batch 0 Loss 0.0011\n",
            "Epoch 292 Loss 0.0013\n",
            "Time taken for 1 epoch 11.399842977523804 sec\n",
            "\n",
            "Epoch 293 Batch 0 Loss 0.0010\n",
            "Epoch 294 Batch 0 Loss 0.0012\n",
            "Epoch 294 Loss 0.0012\n",
            "Time taken for 1 epoch 11.381966829299927 sec\n",
            "\n",
            "Epoch 295 Batch 0 Loss 0.0010\n",
            "Epoch 296 Batch 0 Loss 0.0010\n",
            "Epoch 296 Loss 0.0012\n",
            "Time taken for 1 epoch 11.363637447357178 sec\n",
            "\n",
            "Epoch 297 Batch 0 Loss 0.0011\n",
            "Epoch 298 Batch 0 Loss 0.0010\n",
            "Epoch 298 Loss 0.0011\n",
            "Time taken for 1 epoch 11.348882913589478 sec\n",
            "\n",
            "Epoch 299 Batch 0 Loss 0.0009\n",
            "Epoch 300 Batch 0 Loss 0.0009\n",
            "Epoch 300 Loss 0.0011\n",
            "Time taken for 1 epoch 11.43411135673523 sec\n",
            "\n",
            "Epoch 301 Batch 0 Loss 0.0008\n",
            "Epoch 302 Batch 0 Loss 0.0009\n",
            "Epoch 302 Loss 0.0010\n",
            "Time taken for 1 epoch 11.313002586364746 sec\n",
            "\n",
            "Epoch 303 Batch 0 Loss 0.0009\n",
            "Epoch 304 Batch 0 Loss 0.0008\n",
            "Epoch 304 Loss 0.0010\n",
            "Time taken for 1 epoch 11.360040426254272 sec\n",
            "\n",
            "Epoch 305 Batch 0 Loss 0.0008\n",
            "Epoch 306 Batch 0 Loss 0.0007\n",
            "Epoch 306 Loss 0.0009\n",
            "Time taken for 1 epoch 11.449569463729858 sec\n",
            "\n",
            "Epoch 307 Batch 0 Loss 0.0007\n",
            "Epoch 308 Batch 0 Loss 0.0007\n",
            "Epoch 308 Loss 0.0008\n",
            "Time taken for 1 epoch 11.340606689453125 sec\n",
            "\n",
            "Epoch 309 Batch 0 Loss 0.0007\n",
            "Epoch 310 Batch 0 Loss 0.0007\n",
            "Epoch 310 Loss 0.0008\n",
            "Time taken for 1 epoch 11.37589406967163 sec\n",
            "\n",
            "Epoch 311 Batch 0 Loss 0.0007\n",
            "Epoch 312 Batch 0 Loss 0.0007\n",
            "Epoch 312 Loss 0.0008\n",
            "Time taken for 1 epoch 11.435139894485474 sec\n",
            "\n",
            "Epoch 313 Batch 0 Loss 0.0007\n",
            "Epoch 314 Batch 0 Loss 0.0006\n",
            "Epoch 314 Loss 0.0007\n",
            "Time taken for 1 epoch 11.408320903778076 sec\n",
            "\n",
            "Epoch 315 Batch 0 Loss 0.0006\n",
            "Epoch 316 Batch 0 Loss 0.0006\n",
            "Epoch 316 Loss 0.0007\n",
            "Time taken for 1 epoch 11.382359743118286 sec\n",
            "\n",
            "Epoch 317 Batch 0 Loss 0.0005\n",
            "Epoch 318 Batch 0 Loss 0.0005\n",
            "Epoch 318 Loss 0.0006\n",
            "Time taken for 1 epoch 11.352608919143677 sec\n",
            "\n",
            "Epoch 319 Batch 0 Loss 0.0004\n",
            "Epoch 320 Batch 0 Loss 0.0005\n",
            "Epoch 320 Loss 0.0007\n",
            "Time taken for 1 epoch 11.412461519241333 sec\n",
            "\n",
            "Epoch 321 Batch 0 Loss 0.0005\n",
            "Epoch 322 Batch 0 Loss 0.0005\n",
            "Epoch 322 Loss 0.0006\n",
            "Time taken for 1 epoch 11.459046363830566 sec\n",
            "\n",
            "Epoch 323 Batch 0 Loss 0.0006\n",
            "Epoch 324 Batch 0 Loss 0.0005\n",
            "Epoch 324 Loss 0.0006\n",
            "Time taken for 1 epoch 11.475792407989502 sec\n",
            "\n",
            "Epoch 325 Batch 0 Loss 0.0004\n",
            "Epoch 326 Batch 0 Loss 0.0004\n",
            "Epoch 326 Loss 0.0006\n",
            "Time taken for 1 epoch 11.357157468795776 sec\n",
            "\n",
            "Epoch 327 Batch 0 Loss 0.0004\n",
            "Epoch 328 Batch 0 Loss 0.0005\n",
            "Epoch 328 Loss 0.0005\n",
            "Time taken for 1 epoch 11.33701753616333 sec\n",
            "\n",
            "Epoch 329 Batch 0 Loss 0.0005\n",
            "Epoch 330 Batch 0 Loss 0.0004\n",
            "Epoch 330 Loss 0.0005\n",
            "Time taken for 1 epoch 11.382692575454712 sec\n",
            "\n",
            "Epoch 331 Batch 0 Loss 0.0004\n",
            "Epoch 332 Batch 0 Loss 0.0004\n",
            "Epoch 332 Loss 0.0006\n",
            "Time taken for 1 epoch 11.3264901638031 sec\n",
            "\n",
            "Epoch 333 Batch 0 Loss 0.0011\n",
            "Epoch 334 Batch 0 Loss 0.3006\n",
            "Epoch 334 Loss 0.9071\n",
            "Time taken for 1 epoch 11.395020246505737 sec\n",
            "\n",
            "Epoch 335 Batch 0 Loss 0.7687\n",
            "Epoch 336 Batch 0 Loss 0.3041\n",
            "Epoch 336 Loss 0.3485\n",
            "Time taken for 1 epoch 11.402891397476196 sec\n",
            "\n",
            "Epoch 337 Batch 0 Loss 0.1534\n",
            "Epoch 338 Batch 0 Loss 0.0899\n",
            "Epoch 338 Loss 0.0817\n",
            "Time taken for 1 epoch 11.321688652038574 sec\n",
            "\n",
            "Epoch 339 Batch 0 Loss 0.0391\n",
            "Epoch 340 Batch 0 Loss 0.0216\n",
            "Epoch 340 Loss 0.0220\n",
            "Time taken for 1 epoch 11.41555905342102 sec\n",
            "\n",
            "Epoch 341 Batch 0 Loss 0.0136\n",
            "Epoch 342 Batch 0 Loss 0.0112\n",
            "Epoch 342 Loss 0.0089\n",
            "Time taken for 1 epoch 11.301270723342896 sec\n",
            "\n",
            "Epoch 343 Batch 0 Loss 0.0071\n",
            "Epoch 344 Batch 0 Loss 0.0064\n",
            "Epoch 344 Loss 0.0060\n",
            "Time taken for 1 epoch 11.358488321304321 sec\n",
            "\n",
            "Epoch 345 Batch 0 Loss 0.0053\n",
            "Epoch 346 Batch 0 Loss 0.0049\n",
            "Epoch 346 Loss 0.0048\n",
            "Time taken for 1 epoch 11.356258392333984 sec\n",
            "\n",
            "Epoch 347 Batch 0 Loss 0.0045\n",
            "Epoch 348 Batch 0 Loss 0.0035\n",
            "Epoch 348 Loss 0.0041\n",
            "Time taken for 1 epoch 11.39408254623413 sec\n",
            "\n",
            "Epoch 349 Batch 0 Loss 0.0034\n",
            "Epoch 350 Batch 0 Loss 0.0030\n",
            "Epoch 350 Loss 0.0035\n",
            "Time taken for 1 epoch 11.399585723876953 sec\n",
            "\n",
            "Epoch 351 Batch 0 Loss 0.0027\n",
            "Epoch 352 Batch 0 Loss 0.0028\n",
            "Epoch 352 Loss 0.0031\n",
            "Time taken for 1 epoch 11.386035680770874 sec\n",
            "\n",
            "Epoch 353 Batch 0 Loss 0.0030\n",
            "Epoch 354 Batch 0 Loss 0.0032\n",
            "Epoch 354 Loss 0.0028\n",
            "Time taken for 1 epoch 11.311490535736084 sec\n",
            "\n",
            "Epoch 355 Batch 0 Loss 0.0030\n",
            "Epoch 356 Batch 0 Loss 0.0024\n",
            "Epoch 356 Loss 0.0025\n",
            "Time taken for 1 epoch 11.426918745040894 sec\n",
            "\n",
            "Epoch 357 Batch 0 Loss 0.0022\n",
            "Epoch 358 Batch 0 Loss 0.0030\n",
            "Epoch 358 Loss 0.0023\n",
            "Time taken for 1 epoch 11.401288986206055 sec\n",
            "\n",
            "Epoch 359 Batch 0 Loss 0.0024\n",
            "Epoch 360 Batch 0 Loss 0.0027\n",
            "Epoch 360 Loss 0.0021\n",
            "Time taken for 1 epoch 11.382697105407715 sec\n",
            "\n",
            "Epoch 361 Batch 0 Loss 0.0020\n",
            "Epoch 362 Batch 0 Loss 0.0019\n",
            "Epoch 362 Loss 0.0019\n",
            "Time taken for 1 epoch 11.253968000411987 sec\n",
            "\n",
            "Epoch 363 Batch 0 Loss 0.0018\n",
            "Epoch 364 Batch 0 Loss 0.0017\n",
            "Epoch 364 Loss 0.0018\n",
            "Time taken for 1 epoch 11.38197660446167 sec\n",
            "\n",
            "Epoch 365 Batch 0 Loss 0.0014\n",
            "Epoch 366 Batch 0 Loss 0.0017\n",
            "Epoch 366 Loss 0.0017\n",
            "Time taken for 1 epoch 11.416000366210938 sec\n",
            "\n",
            "Epoch 367 Batch 0 Loss 0.0017\n",
            "Epoch 368 Batch 0 Loss 0.0014\n",
            "Epoch 368 Loss 0.0016\n",
            "Time taken for 1 epoch 11.379724740982056 sec\n",
            "\n",
            "Epoch 369 Batch 0 Loss 0.0016\n",
            "Epoch 370 Batch 0 Loss 0.0012\n",
            "Epoch 370 Loss 0.0015\n",
            "Time taken for 1 epoch 11.362337350845337 sec\n",
            "\n",
            "Epoch 371 Batch 0 Loss 0.0013\n",
            "Epoch 372 Batch 0 Loss 0.0012\n",
            "Epoch 372 Loss 0.0013\n",
            "Time taken for 1 epoch 11.359887599945068 sec\n",
            "\n",
            "Epoch 373 Batch 0 Loss 0.0012\n",
            "Epoch 374 Batch 0 Loss 0.0010\n",
            "Epoch 374 Loss 0.0013\n",
            "Time taken for 1 epoch 11.408057451248169 sec\n",
            "\n",
            "Epoch 375 Batch 0 Loss 0.0011\n",
            "Epoch 376 Batch 0 Loss 0.0018\n",
            "Epoch 376 Loss 0.0012\n",
            "Time taken for 1 epoch 11.283036470413208 sec\n",
            "\n",
            "Epoch 377 Batch 0 Loss 0.0011\n",
            "Epoch 378 Batch 0 Loss 0.0011\n",
            "Epoch 378 Loss 0.0011\n",
            "Time taken for 1 epoch 11.343436479568481 sec\n",
            "\n",
            "Epoch 379 Batch 0 Loss 0.0011\n",
            "Epoch 380 Batch 0 Loss 0.0008\n",
            "Epoch 380 Loss 0.0010\n",
            "Time taken for 1 epoch 11.375715970993042 sec\n",
            "\n",
            "Epoch 381 Batch 0 Loss 0.0007\n",
            "Epoch 382 Batch 0 Loss 0.0009\n",
            "Epoch 382 Loss 0.0010\n",
            "Time taken for 1 epoch 11.273242473602295 sec\n",
            "\n",
            "Epoch 383 Batch 0 Loss 0.0009\n",
            "Epoch 384 Batch 0 Loss 0.0008\n",
            "Epoch 384 Loss 0.0010\n",
            "Time taken for 1 epoch 11.414834260940552 sec\n",
            "\n",
            "Epoch 385 Batch 0 Loss 0.0007\n",
            "Epoch 386 Batch 0 Loss 0.0008\n",
            "Epoch 386 Loss 0.0009\n",
            "Time taken for 1 epoch 20.60024929046631 sec\n",
            "\n",
            "Epoch 387 Batch 0 Loss 0.0007\n",
            "Epoch 388 Batch 0 Loss 0.0009\n",
            "Epoch 388 Loss 0.0009\n",
            "Time taken for 1 epoch 11.43418264389038 sec\n",
            "\n",
            "Epoch 389 Batch 0 Loss 0.0007\n",
            "Epoch 390 Batch 0 Loss 0.0007\n",
            "Epoch 390 Loss 0.0009\n",
            "Time taken for 1 epoch 11.387716293334961 sec\n",
            "\n",
            "Epoch 391 Batch 0 Loss 0.0007\n",
            "Epoch 392 Batch 0 Loss 0.0007\n",
            "Epoch 392 Loss 0.0008\n",
            "Time taken for 1 epoch 11.418296337127686 sec\n",
            "\n",
            "Epoch 393 Batch 0 Loss 0.0006\n",
            "Epoch 394 Batch 0 Loss 0.0007\n",
            "Epoch 394 Loss 0.0007\n",
            "Time taken for 1 epoch 11.375717401504517 sec\n",
            "\n",
            "Epoch 395 Batch 0 Loss 0.0006\n",
            "Epoch 396 Batch 0 Loss 0.0005\n",
            "Epoch 396 Loss 0.0007\n",
            "Time taken for 1 epoch 20.596017122268677 sec\n",
            "\n",
            "Epoch 397 Batch 0 Loss 0.0006\n",
            "Epoch 398 Batch 0 Loss 0.0006\n",
            "Epoch 398 Loss 0.0007\n",
            "Time taken for 1 epoch 11.398845195770264 sec\n",
            "\n",
            "Epoch 399 Batch 0 Loss 0.0006\n",
            "Epoch 400 Batch 0 Loss 0.0013\n",
            "Epoch 400 Loss 0.0007\n",
            "Time taken for 1 epoch 11.45121693611145 sec\n",
            "\n",
            "Epoch 401 Batch 0 Loss 0.0005\n",
            "Epoch 402 Batch 0 Loss 0.0009\n",
            "Epoch 402 Loss 0.0006\n",
            "Time taken for 1 epoch 11.353300333023071 sec\n",
            "\n",
            "Epoch 403 Batch 0 Loss 0.0006\n",
            "Epoch 404 Batch 0 Loss 0.0004\n",
            "Epoch 404 Loss 0.0006\n",
            "Time taken for 1 epoch 11.35558533668518 sec\n",
            "\n",
            "Epoch 405 Batch 0 Loss 0.0004\n",
            "Epoch 406 Batch 0 Loss 0.0004\n",
            "Epoch 406 Loss 0.0006\n",
            "Time taken for 1 epoch 11.3421630859375 sec\n",
            "\n",
            "Epoch 407 Batch 0 Loss 0.0004\n",
            "Epoch 408 Batch 0 Loss 0.0008\n",
            "Epoch 408 Loss 0.0007\n",
            "Time taken for 1 epoch 11.378218650817871 sec\n",
            "\n",
            "Epoch 409 Batch 0 Loss 0.0004\n",
            "Epoch 410 Batch 0 Loss 0.0004\n",
            "Epoch 410 Loss 0.0006\n",
            "Time taken for 1 epoch 11.377806663513184 sec\n",
            "\n",
            "Epoch 411 Batch 0 Loss 0.0004\n",
            "Epoch 412 Batch 0 Loss 0.0004\n",
            "Epoch 412 Loss 0.0006\n",
            "Time taken for 1 epoch 11.488999843597412 sec\n",
            "\n",
            "Epoch 413 Batch 0 Loss 0.0004\n",
            "Epoch 414 Batch 0 Loss 0.0004\n",
            "Epoch 414 Loss 0.0005\n",
            "Time taken for 1 epoch 11.401077032089233 sec\n",
            "\n",
            "Epoch 415 Batch 0 Loss 0.0004\n",
            "Epoch 416 Batch 0 Loss 0.0003\n",
            "Epoch 416 Loss 0.0006\n",
            "Time taken for 1 epoch 11.388830661773682 sec\n",
            "\n",
            "Epoch 417 Batch 0 Loss 0.0003\n",
            "Epoch 418 Batch 0 Loss 0.0003\n",
            "Epoch 418 Loss 0.0005\n",
            "Time taken for 1 epoch 11.344560146331787 sec\n",
            "\n",
            "Epoch 419 Batch 0 Loss 0.0003\n",
            "Epoch 420 Batch 0 Loss 0.0004\n",
            "Epoch 420 Loss 0.0006\n",
            "Time taken for 1 epoch 11.298673868179321 sec\n",
            "\n",
            "Epoch 421 Batch 0 Loss 0.0003\n",
            "Epoch 422 Batch 0 Loss 0.0003\n",
            "Epoch 422 Loss 0.0005\n",
            "Time taken for 1 epoch 11.393860101699829 sec\n",
            "\n",
            "Epoch 423 Batch 0 Loss 0.0002\n",
            "Epoch 424 Batch 0 Loss 0.0003\n",
            "Epoch 424 Loss 0.0005\n",
            "Time taken for 1 epoch 11.421653270721436 sec\n",
            "\n",
            "Epoch 425 Batch 0 Loss 0.0003\n",
            "Epoch 426 Batch 0 Loss 0.0003\n",
            "Epoch 426 Loss 0.0004\n",
            "Time taken for 1 epoch 11.44570779800415 sec\n",
            "\n",
            "Epoch 427 Batch 0 Loss 0.0003\n",
            "Epoch 428 Batch 0 Loss 0.0003\n",
            "Epoch 428 Loss 0.0004\n",
            "Time taken for 1 epoch 11.383592128753662 sec\n",
            "\n",
            "Epoch 429 Batch 0 Loss 0.0003\n",
            "Epoch 430 Batch 0 Loss 0.0015\n",
            "Epoch 430 Loss 0.0004\n",
            "Time taken for 1 epoch 11.354775190353394 sec\n",
            "\n",
            "Epoch 431 Batch 0 Loss 0.0003\n",
            "Epoch 432 Batch 0 Loss 0.0003\n",
            "Epoch 432 Loss 0.0005\n",
            "Time taken for 1 epoch 11.409883737564087 sec\n",
            "\n",
            "Epoch 433 Batch 0 Loss 0.0002\n",
            "Epoch 434 Batch 0 Loss 0.0003\n",
            "Epoch 434 Loss 0.0004\n",
            "Time taken for 1 epoch 11.411986112594604 sec\n",
            "\n",
            "Epoch 435 Batch 0 Loss 0.0002\n",
            "Epoch 436 Batch 0 Loss 0.0149\n",
            "Epoch 436 Loss 0.6346\n",
            "Time taken for 1 epoch 11.32213020324707 sec\n",
            "\n",
            "Epoch 437 Batch 0 Loss 0.8462\n",
            "Epoch 438 Batch 0 Loss 0.4145\n",
            "Epoch 438 Loss 0.3594\n",
            "Time taken for 1 epoch 11.309518575668335 sec\n",
            "\n",
            "Epoch 439 Batch 0 Loss 0.1963\n",
            "Epoch 440 Batch 0 Loss 0.0896\n",
            "Epoch 440 Loss 0.0784\n",
            "Time taken for 1 epoch 11.400734186172485 sec\n",
            "\n",
            "Epoch 441 Batch 0 Loss 0.0441\n",
            "Epoch 442 Batch 0 Loss 0.0223\n",
            "Epoch 442 Loss 0.0189\n",
            "Time taken for 1 epoch 11.368119478225708 sec\n",
            "\n",
            "Epoch 443 Batch 0 Loss 0.0128\n",
            "Epoch 444 Batch 0 Loss 0.0069\n",
            "Epoch 444 Loss 0.0071\n",
            "Time taken for 1 epoch 11.386931657791138 sec\n",
            "\n",
            "Epoch 445 Batch 0 Loss 0.0046\n",
            "Epoch 446 Batch 0 Loss 0.0047\n",
            "Epoch 446 Loss 0.0046\n",
            "Time taken for 1 epoch 11.42255711555481 sec\n",
            "\n",
            "Epoch 447 Batch 0 Loss 0.0043\n",
            "Epoch 448 Batch 0 Loss 0.0041\n",
            "Epoch 448 Loss 0.0037\n",
            "Time taken for 1 epoch 11.370790243148804 sec\n",
            "\n",
            "Epoch 449 Batch 0 Loss 0.0034\n",
            "Epoch 450 Batch 0 Loss 0.0026\n",
            "Epoch 450 Loss 0.0031\n",
            "Time taken for 1 epoch 11.41074275970459 sec\n",
            "\n",
            "Epoch 451 Batch 0 Loss 0.0029\n",
            "Epoch 452 Batch 0 Loss 0.0028\n",
            "Epoch 452 Loss 0.0028\n",
            "Time taken for 1 epoch 11.445858716964722 sec\n",
            "\n",
            "Epoch 453 Batch 0 Loss 0.0023\n",
            "Epoch 454 Batch 0 Loss 0.0023\n",
            "Epoch 454 Loss 0.0024\n",
            "Time taken for 1 epoch 11.405762910842896 sec\n",
            "\n",
            "Epoch 455 Batch 0 Loss 0.0022\n",
            "Epoch 456 Batch 0 Loss 0.0023\n",
            "Epoch 456 Loss 0.0022\n",
            "Time taken for 1 epoch 11.457717895507812 sec\n",
            "\n",
            "Epoch 457 Batch 0 Loss 0.0023\n",
            "Epoch 458 Batch 0 Loss 0.0018\n",
            "Epoch 458 Loss 0.0020\n",
            "Time taken for 1 epoch 11.431235790252686 sec\n",
            "\n",
            "Epoch 459 Batch 0 Loss 0.0016\n",
            "Epoch 460 Batch 0 Loss 0.0020\n",
            "Epoch 460 Loss 0.0018\n",
            "Time taken for 1 epoch 11.36170768737793 sec\n",
            "\n",
            "Epoch 461 Batch 0 Loss 0.0015\n",
            "Epoch 462 Batch 0 Loss 0.0020\n",
            "Epoch 462 Loss 0.0017\n",
            "Time taken for 1 epoch 11.365339517593384 sec\n",
            "\n",
            "Epoch 463 Batch 0 Loss 0.0013\n",
            "Epoch 464 Batch 0 Loss 0.0015\n",
            "Epoch 464 Loss 0.0015\n",
            "Time taken for 1 epoch 11.392759323120117 sec\n",
            "\n",
            "Epoch 465 Batch 0 Loss 0.0014\n",
            "Epoch 466 Batch 0 Loss 0.0013\n",
            "Epoch 466 Loss 0.0014\n",
            "Time taken for 1 epoch 11.459296703338623 sec\n",
            "\n",
            "Epoch 467 Batch 0 Loss 0.0017\n",
            "Epoch 468 Batch 0 Loss 0.0012\n",
            "Epoch 468 Loss 0.0013\n",
            "Time taken for 1 epoch 11.361907005310059 sec\n",
            "\n",
            "Epoch 469 Batch 0 Loss 0.0010\n",
            "Epoch 470 Batch 0 Loss 0.0014\n",
            "Epoch 470 Loss 0.0012\n",
            "Time taken for 1 epoch 11.401431322097778 sec\n",
            "\n",
            "Epoch 471 Batch 0 Loss 0.0010\n",
            "Epoch 472 Batch 0 Loss 0.0010\n",
            "Epoch 472 Loss 0.0012\n",
            "Time taken for 1 epoch 11.385621786117554 sec\n",
            "\n",
            "Epoch 473 Batch 0 Loss 0.0011\n",
            "Epoch 474 Batch 0 Loss 0.0012\n",
            "Epoch 474 Loss 0.0011\n",
            "Time taken for 1 epoch 11.469762802124023 sec\n",
            "\n",
            "Epoch 475 Batch 0 Loss 0.0012\n",
            "Epoch 476 Batch 0 Loss 0.0009\n",
            "Epoch 476 Loss 0.0010\n",
            "Time taken for 1 epoch 11.38059139251709 sec\n",
            "\n",
            "Epoch 477 Batch 0 Loss 0.0010\n",
            "Epoch 478 Batch 0 Loss 0.0008\n",
            "Epoch 478 Loss 0.0010\n",
            "Time taken for 1 epoch 11.451301574707031 sec\n",
            "\n",
            "Epoch 479 Batch 0 Loss 0.0010\n",
            "Epoch 480 Batch 0 Loss 0.0007\n",
            "Epoch 480 Loss 0.0009\n",
            "Time taken for 1 epoch 11.378162384033203 sec\n",
            "\n",
            "Epoch 481 Batch 0 Loss 0.0008\n",
            "Epoch 482 Batch 0 Loss 0.0007\n",
            "Epoch 482 Loss 0.0009\n",
            "Time taken for 1 epoch 11.404359579086304 sec\n",
            "\n",
            "Epoch 483 Batch 0 Loss 0.0008\n",
            "Epoch 484 Batch 0 Loss 0.0007\n",
            "Epoch 484 Loss 0.0008\n",
            "Time taken for 1 epoch 11.357611894607544 sec\n",
            "\n",
            "Epoch 485 Batch 0 Loss 0.0007\n",
            "Epoch 486 Batch 0 Loss 0.0008\n",
            "Epoch 486 Loss 0.0008\n",
            "Time taken for 1 epoch 11.343040466308594 sec\n",
            "\n",
            "Epoch 487 Batch 0 Loss 0.0009\n",
            "Epoch 488 Batch 0 Loss 0.0006\n",
            "Epoch 488 Loss 0.0008\n",
            "Time taken for 1 epoch 11.37663722038269 sec\n",
            "\n",
            "Epoch 489 Batch 0 Loss 0.0007\n",
            "Epoch 490 Batch 0 Loss 0.0006\n",
            "Epoch 490 Loss 0.0007\n",
            "Time taken for 1 epoch 11.394190788269043 sec\n",
            "\n",
            "Epoch 491 Batch 0 Loss 0.0006\n",
            "Epoch 492 Batch 0 Loss 0.0005\n",
            "Epoch 492 Loss 0.0007\n",
            "Time taken for 1 epoch 11.366273403167725 sec\n",
            "\n",
            "Epoch 493 Batch 0 Loss 0.0005\n",
            "Epoch 494 Batch 0 Loss 0.0005\n",
            "Epoch 494 Loss 0.0007\n",
            "Time taken for 1 epoch 11.379863977432251 sec\n",
            "\n",
            "Epoch 495 Batch 0 Loss 0.0005\n",
            "Epoch 496 Batch 0 Loss 0.0005\n",
            "Epoch 496 Loss 0.0006\n",
            "Time taken for 1 epoch 11.372320175170898 sec\n",
            "\n",
            "Epoch 497 Batch 0 Loss 0.0006\n",
            "Epoch 498 Batch 0 Loss 0.0006\n",
            "Epoch 498 Loss 0.0006\n",
            "Time taken for 1 epoch 11.358670711517334 sec\n",
            "\n",
            "Epoch 499 Batch 0 Loss 0.0005\n",
            "Epoch 500 Batch 0 Loss 0.0004\n",
            "Epoch 500 Loss 0.0006\n",
            "Time taken for 1 epoch 11.372189044952393 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff98a4eb"
      },
      "source": [
        "### 9. Evaluate Sentence"
      ],
      "id": "ff98a4eb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a682a784"
      },
      "source": [
        "import unicodedata\n",
        "import re\n",
        "import string\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "#     w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "#     w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    w = w.rstrip().strip()\n",
        "    return w"
      ],
      "id": "a682a784",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e44c351"
      },
      "source": [
        "def evaluate_sentence(sentence):\n",
        "#     sentence = preprocess_sentence(sentence)\n",
        "    \n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_inp,\n",
        "                                                          padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    inference_batch_size = inputs.shape[0]\n",
        "    result = ''\n",
        "\n",
        "    enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "    enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "    dec_h = enc_h\n",
        "    dec_c = enc_c\n",
        "\n",
        "    start_tokens = tf.fill([inference_batch_size], targ_lang.word_index['<start>'])\n",
        "    end_token = targ_lang.word_index['<end>']\n",
        "\n",
        "    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "    # Instantiate BasicDecoder object\n",
        "    decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
        "    # Setup Memory in decoder stack\n",
        "    decoder.attention_mechanism.setup_memory(enc_out)\n",
        "\n",
        "    # set decoder_initial_state\n",
        "    decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
        "\n",
        "\n",
        "    ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "    ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "    ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "    decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "\n",
        "    outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
        "    return outputs.sample_id.numpy()\n",
        "\n",
        "def translate(sentence):\n",
        "    result = evaluate_sentence(sentence)\n",
        "    print(result)\n",
        "    result = targ_lang.sequences_to_texts(result)\n",
        "    print('Input: %s' % (sentence))\n",
        "    print('Predicted translation: {}'.format(result))"
      ],
      "id": "4e44c351",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a155785f",
        "outputId": "6f9fde8c-aa45-49ff-abcd-ca42779b89f9"
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "id": "a155785f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0734a76410>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c96fdba7",
        "outputId": "107cd28d-ac1d-4d88-cc01-39eec46fdca0"
      },
      "source": [
        "translate(u'वर्षमा कुल पैठारीमा औद्योगिक सामानहरुको वाहुल्यता रह्यो')"
      ],
      "id": "c96fdba7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  1  65  65 105  26 453   3]]\n",
            "Input: वर्षमा कुल पैठारीमा औद्योगिक सामानहरुको वाहुल्यता रह्यो\n",
            "Predicted translation: ['the economic economic population has started <end>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78973ddf"
      },
      "source": [
        "### 10. Test"
      ],
      "id": "78973ddf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecd269ac"
      },
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model(raw_dataset):\n",
        "    actual, predicted = list(), list()\n",
        "    \n",
        "    for i, source in enumerate(raw_dataset):\n",
        "        raw_target, raw_src = source[0], source[1]\n",
        "        result = evaluate_sentence(raw_src)\n",
        "        translation = targ_lang.sequences_to_texts(result)[0]\n",
        "        \n",
        "        if i < 10:\n",
        "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_src, raw_target, translation))    \n",
        "            \n",
        "        actual.append([raw_target.split()])\n",
        "        predicted.append(translation.split())\n",
        "        \n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "id": "ecd269ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WTFiXrHwLxNl",
        "outputId": "fe7b7cbf-4893-4cb2-9712-0566dea67d96"
      },
      "source": [
        "train[0][1]"
      ],
      "id": "WTFiXrHwLxNl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'सहरमा निजी वाहन प्रयोग गर्न प्रतिबन्ध लगाइएको थियो।'"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e86fd8af",
        "outputId": "ff4b5fdf-49db-4d6b-ee52-7268b81cfc1e"
      },
      "source": [
        "# test on some training sequences\n",
        "print('train')\n",
        "evaluate_model(train)\n",
        "# test on some test sequences\n",
        "print('test')\n",
        "evaluate_model(test)"
      ],
      "id": "e86fd8af",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train\n",
            "src=[सहरमा निजी वाहन प्रयोग गर्न प्रतिबन्ध लगाइएको थियो।], target=[private vehicle use was banned in the city], predicted=[private vehicle use was banned in the export <end>]\n",
            "src=[who ले मानिसहरूलाई नधोएको हातले आँखा नाक वा मुख छुनुबाट टाढा रहन सल्लाह दिन्छ।], target=[the who advises people to avoid touching the eyes nose or mouth with unwashed hands], predicted=[the who advises people who nose with the eyes nose and mouth with unwashed hands <end>]\n",
            "src=[covid को crp स्तर र गम्भीरता र रोग पूर्वानुमानको सहसम्बन्ध पनि प्रस्तावित गरिएको छ।], target=[the correlation of crp level to the severity and prognosis of covid has also been proposed], predicted=[the correlation of crp level to the severity and prognosis of covid has been proposed <end>]\n",
            "src=[यो बृद्धि दर विगत तीन दशकको बृद्धि दर भन्दा बढी छ।], target=[this growth rate is higher than that of the last three decades], predicted=[this growth rate is higher than that if the growth rate of growth in per increase is too <end>]\n",
            "src=[प्राथमिक कक्षाकक्षा  देखि  सम्म तहमा  सालमा शिक्षक र विद्यार्थी अनुपात  रहेकोमा  को वैशाखसम्ममा उक्त अनुपातमा बृद्धिभई  रहन गयो।], target=[while teacherstudent ration was  at the primary class class   level in  it increased to  by april ], predicted=[while teacherstudent ration was  at the primary class class   level in  it increased to  by april  <end>]\n",
            "src=[पञ्चायतीकालमा निर्माण सुरु गरिएको पूर्व  पश्चिम राजमार्ग मुलुकको विकासका लागि वहुआयामिक आयोजना थियो ।], target=[the construction of the eastwest highway which was initiated in the panchayat regime was a multidimensional project for national development], predicted=[the construction of the eastwest highway which was initiated in the panchayat regime was a multidimensional project for national development <end>]\n",
            "src=[त्यस्तै खोजहरू एक परिवार समूह र लक्षण विहीन व्यक्तिबाट सङ्क्रमणका कारण निर्मित समूहका दुई भर्खरका अध्ययनहरूमा रिपोर्ट गरिएका थिए।], target=[similar findings were reported in two recent studies of a family cluster and a cluster caused by transmission from an asymptomatic individual], predicted=[similar findings were reported in two recent studies of a family cluster and a cluster caused by transmission from an asymptomatic individual <end>]\n",
            "src=[न्युयोर्क इथाकामा कर्नेल विश्वविद्यालयका विद्यार्थीहरूले टम्पकिन्स काउन्टीमा हप्तामा कम्तीमा  मिलियन खर्च गर्थे।], target=[in ithaca new york cornell university students spent at least  million a week in tompkins county], predicted=[in ithaca new york cornell university students spent at least  million a week in tompkins county <end>]\n",
            "src=[त्यसले गर्दा योजना प्रक्रिया अनुत्तरदायी किसिमले सञ्चालन गरियो र जनसहभागिताको सर्वथा अभाव रह्यो।], target=[as a result the planning process lacked accountability and was unable to induce genuine popular participation], predicted=[as a result the planning process lacked accountability and was unable to induce genuine popular participation <end>]\n",
            "src=[ मार्च सम्‍ममा विश्वव्यापी रुपमा  विद्यार्थी विद्यालय बन्द हुनाले प्रभावित भए जसअनुरुप  देशहरूमा राष्ट्रव्यापी बन्द र  देशहरूमा स्थानीय रुपमा बन्द ले  मिलियन बालबालिका र युवाहरूलाई प्रभाव पारेको छ।], target=[by  march  of students worldwide were affected by school closures corresponding to nationwide closures in  countries and local closures in  countries affecting  million children and youth], predicted=[by  march  of students worldwide were affected by school closures corresponding to nationwide closures in  countries and local closures in  countries affecting  million children and youth <end>]\n",
            "BLEU-1: 0.825670\n",
            "BLEU-2: 0.801728\n",
            "BLEU-3: 0.805319\n",
            "BLEU-4: 0.774398\n",
            "test\n",
            "src=[चरण ii परीक्षणहरू प्रभावकारिताको प्रारम्भिक पठन स्थापना गर्न प्रयोग गरिन्छन् र nce द्वारा लक्षित रोग भएका मानिसहरूको थोरै सङ्ख्यामा थप सुरक्षा अन्वेषण गरिन्छ।], target=[phase ii trials are used to establish an initial reading of efficacy and further explore safety in small numbers of people having the disease targeted by the nce], predicted=[testing of example respiratory providers is after after after the spread of a small virus that which is available after a significant risk of infection <end>]\n",
            "src=[संयुक्त अधिराज्य सरकार  यात्रुहरूको लागि विदेश तथा राष्ट्रमण्डल कार्यालयको सुझाव], target=[united kingdom government  foreign and commonwealth office advice for travellers], predicted=[this was  as that have had been reported across china china <end>]\n",
            "src=[लक्षणहरूको अनावरणदेखि प्रारम्भ हुने समय सामान्यतया लगभग पाँच दिनको हुन्छ तर दुईदेखि चौध दिनको दायरामा हुन सक्छन्।], target=[the time from exposure to onset of symptoms is typically around five days but may range from two to fourteen days], predicted=[the sarscov is in favour of sarscov is  days for  may  days with  days <end>]\n",
            "src=[एन्टिबडीहरू b लिम्फोसाइटहरूले रोगजनक र अन्य बाह्य वस्तुहरूसँग लड्न उत्पादन गरेका इम्युनोग्लोबिन हुन् ig र तिनीहरूले रोगजनकमा अद्वितीय अणुहरूको पहिचान गर्छन् र तिनीहरूलाई सिधै तटस्थ पार्छन्।], target=[antibodies are an immunoglobulin ig produced by b lymphocytes to fight pathogens and other foreign objects and they recognize unique molecules in the pathogens and neutralize them directly], predicted=[in fy the fact from the s unit which are necessary to be a large uefa including which infects all persistent and action across the antibody cold <end>]\n",
            "src=[त्यसको अलावा निजी क्षेत्रलाई प्रदान गरिने उपयुक्त वातावरणबाट गार्हस्थ्य बचत बढाउनमा समेत मद्दत मिल्नेछ।], target=[besides the creation of a favorable environment for private sector will also stimulate growth in domestic savings], predicted=[besides the creation of a favorable environment for private sector will also stimulate growth in domestic savings <end>]\n",
            "src=[यदि उत्तर हो आउँछ भने त्यसोभए तपाईँ धेरै नजिक हुनुहुन्छ।], target=[if the answer is yes then youre too close], predicted=[what people in your time <end>]\n",
            "src=[हामीले विशेष कर्मचारी बैठकको लागि अर्को बिहीबार  utc pt सम्म तपाईँको पात्रोहरूमा आमन्त्रणा पठाउनेछौँ।], target=[we’ll be sending an invitation to your calendars for next thursday  utc pt for a special staff meeting], predicted=[if you want of multiple countries especially of extra countries especially in your countries especially <end>]\n",
            "src=[एकातर्फ खानेपानी शुद्धिकरणको अभाव थियो भने अर्कोतर्फ खानेपानीमा प्रसस्त चुहावट देखियो।], target=[on the one hand there was a lack of hygienic supply of water and on the other there was plenty of leakage in the drinking water], predicted=[the city of wuhan areas was quickly use then and students in the population <end>]\n",
            "src=[यस सम्बन्धमा sarscov ले अन्य छ hcovs को सामान्य प्रवृत्तिहरूको अनुसरण गर्दछ।], target=[in this regard sarscov follows the general trend of the other six hcovs], predicted=[what patients with the sarscov history with the intermediate and sarscov <end>]\n",
            "src=[अधिकांश प्रभावकारी अवयवहरू अज्ञात रहन्छन् वा अस्पष्ट हुन्छन् किनकि यस्ता अवयव वा तिनीहरूको सर्वोत्तम संयोजनलाई निकाल्न र प्रमाणित गर्न कठिन हुन्छ।], target=[most of the effective components remain unknown or are vague as it is difficult to extract and verify such components or their optimal combinations], predicted=[medical handwashing also known as well as is more updated and a practice of a effective history on public transmission of antiviral therapy to be known <end>]\n",
            "BLEU-1: 0.378832\n",
            "BLEU-2: 0.306991\n",
            "BLEU-3: 0.315862\n",
            "BLEU-4: 0.262754\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO5tq3Bxv2Sr"
      },
      "source": [
        ""
      ],
      "id": "CO5tq3Bxv2Sr",
      "execution_count": null,
      "outputs": []
    }
  ]
}